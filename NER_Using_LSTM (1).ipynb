{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KWWTUeX_tgRp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import os.path\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VC5zsfke0oXc"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U2ODfpbntvCe"
      },
      "outputs": [],
      "source": [
        "\n",
        "SEED = 544\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Credit: From PyTorch's documentation\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qFDZwgfV07mv"
      },
      "outputs": [],
      "source": [
        "def reduceTag(tag,flag=False):\n",
        "  if flag:\n",
        "    return tag\n",
        "  else:\n",
        "    if tag == 'O':\n",
        "      return tag\n",
        "    else:\n",
        "      return 'X'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OUx5tu2Ety0Y"
      },
      "outputs": [],
      "source": [
        "def read_data(fname, test_dataset=False):\n",
        "    sentences = []\n",
        "    with open(fname, 'r') as f:\n",
        "        lines = [line.strip() for line in f.readlines()]\n",
        "        sentence_words = []\n",
        "        sentence_tags = []\n",
        "        for line in lines:\n",
        "            if line:\n",
        "                # test data has only index and word\n",
        "                if test_dataset:\n",
        "                    word = line\n",
        "                    sentence_words.append(word)\n",
        "                # train/dev data has index, word, and tag\n",
        "                else:\n",
        "                    if len(line) < 2:\n",
        "                      continue\n",
        "                    word, tag = line.split()\n",
        "                    sentence_words.append(word)\n",
        "                    sentence_tags.append(reduceTag(tag))\n",
        "            else:\n",
        "                # Create a sentence upon reaching an empty new line\n",
        "                if test_dataset:\n",
        "                    sentences.append(sentence_words)\n",
        "                else:\n",
        "                    sentences.append((sentence_words, sentence_tags))\n",
        "                sentence_words = []\n",
        "                sentence_tags = []\n",
        "        # Create a sentence for the last sentence in the document\n",
        "        # incase it missed a newline in the document at the end\n",
        "        if len(sentence_words) > 0:\n",
        "            if test_dataset:\n",
        "                sentences.append(sentence_words)\n",
        "            else:\n",
        "                sentences.append((sentence_words, sentence_tags))\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z8cAdJdNt7Ua",
        "outputId": "9de7e24b-1df5-49b2-9071-002e27fbabcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rLyIcMMXt7cT"
      },
      "outputs": [],
      "source": [
        "train_Path = '/content/drive/MyDrive/data/train.tagged'\n",
        "test_Path = '/content/drive/MyDrive/data/dev.tagged'\n",
        "eval_Path = '/content/drive/MyDrive/data/test.untagged'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QezzYjH-uXaj"
      },
      "outputs": [],
      "source": [
        "# Read all datasets given\n",
        "train_data = read_data(train_Path)\n",
        "dev_data = read_data(test_Path)\n",
        "test_data = read_data(eval_Path, test_dataset=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o5T0OGA_vltu"
      },
      "outputs": [],
      "source": [
        "# Converts a sequence of words to a series of indices as given by the to_ix mapping (word -> index).\n",
        "def prepare_sequence(seq, to_ix, use_unk=False):\n",
        "    if use_unk:\n",
        "        indices = [to_ix[w] if w in to_ix else to_ix[''] for w in seq]\n",
        "    else:\n",
        "        indices = [to_ix[w] for w in seq]\n",
        "    return indices\n",
        "\n",
        "# Returns the spelling features for each word in the sentence.\n",
        "# Currently there are only 5 features:\n",
        "# 0 - PAD = special token used for the word ''\n",
        "# 1 - ALL_LOWER = when the word is all lower case, such as 'cat'\n",
        "# 2 - ALL_UPPER = when the word is all upper case, such as 'IBM'\n",
        "# 3 - FIRST_UPPER = when the first character is capitalized, such as 'John'\n",
        "# 4 - OTHERS = all other words that did not fit the categories above\n",
        "def get_spelling_feature(sentence):\n",
        "    result = []\n",
        "    for word in sentence:\n",
        "        # PAD = 0\n",
        "        if word == '':\n",
        "            result.append(0)\n",
        "        ## ALL LOWER = 1\n",
        "        elif word.islower():\n",
        "            result.append(1)\n",
        "        # ALL UPPER = 2\n",
        "        elif word.isupper():\n",
        "            result.append(2)\n",
        "        # FIRST UPPER = 3\n",
        "        elif word[0].isupper():\n",
        "            result.append(3)\n",
        "        # OTHERS = 4\n",
        "        else:\n",
        "            result.append(4)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0a9i4FR-vmS5"
      },
      "outputs": [],
      "source": [
        "# The NERDataset is responsible for converting the data, as retrieved from read_data(), into PyTorch tensors of indices\n",
        "# It will pad all sentences to the same length and convert words to indices using a vocabulary lookup\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        # Retrieves longest sentence, for padding\n",
        "        max_sentence_len = max([len(sentence) for sentence, tags in data])\n",
        "        self.X = []\n",
        "        self.X_original = []\n",
        "        self.y = []\n",
        "        self.X_spelling = []\n",
        "        \n",
        "        for sentence, tags in data:\n",
        "            # Pad the sentences to the same length\n",
        "            padded_sentence = sentence.copy()\n",
        "            padded_tags = tags.copy()\n",
        "            while len(padded_sentence) < max_sentence_len:\n",
        "                padded_sentence.append('')\n",
        "                padded_tags.append('')\n",
        "            # Convert to indices\n",
        "            transformed_sentence = prepare_sequence(padded_sentence, word_to_ix, use_unk=True)\n",
        "            transformed_tags = prepare_sequence(padded_tags, tag_to_ix)\n",
        "            # Get spelling indices\n",
        "            spelling_sentence = get_spelling_feature(padded_sentence)\n",
        "            # Add to dataset\n",
        "            self.X.append(transformed_sentence)\n",
        "            self.X_original.append(padded_sentence)\n",
        "            self.y.append(transformed_tags)\n",
        "            self.X_spelling.append(spelling_sentence)\n",
        "            \n",
        "        self.X = torch.from_numpy(np.array(self.X, dtype=np.int64)).to(device)\n",
        "        self.y = torch.from_numpy(np.array(self.y, dtype=np.int64)).to(device)\n",
        "        self.X_spelling = torch.from_numpy(np.array(self.X_spelling, dtype=np.int64)).to(device)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index], self.X_original[index], self.X_spelling[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "piUIfOr8vyp7"
      },
      "outputs": [],
      "source": [
        "VOCAB_THRESHOLD = 0\n",
        "\n",
        "# Generate vocab\n",
        "words_freq = defaultdict(int)\n",
        "for sentence, tags in train_data:\n",
        "    for word in sentence:\n",
        "        words_freq[word] += 1\n",
        "        \n",
        "vocab = {key for key, val in words_freq.items() if val >= VOCAB_THRESHOLD}\n",
        "\n",
        "# Generate word/tag to index mappings\n",
        "word_to_ix = {'': 0, '': 1}\n",
        "tag_to_ix = {'': 0}\n",
        "for sentence, tags in train_data:\n",
        "    for word in sentence:\n",
        "        if word not in vocab:\n",
        "            word = ''\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "            \n",
        "# Generate index to word/tag mappings\n",
        "ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Calculate the size of vocabulary & tags\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "TAGS_SIZE = len(tag_to_ix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHNQcoeSwAha"
      },
      "source": [
        "# Utility Functions for Prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using GloVe word embeddings\n"
      ],
      "metadata": {
        "id": "UPcXEqaNarMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1\n",
        "\n",
        "EMBEDDING_DIM = 200\n",
        "LSTM_HIDDEN_DIM = 256\n",
        "LSTM_DROPOUT = 0.25\n",
        "LINEAR_DIM = 164\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "ELU_ALPHA = 0.5\n",
        "\n",
        "SCHEDULER_STEP_SIZE = 5\n",
        "SCHEDULER_GAMMA = 0.5\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "SPELLING_EMBEDDING_DIM = 15"
      ],
      "metadata": {
        "id": "fqXQgs8GapIJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence,tag in train_data:\n",
        "  print(sentence)\n",
        "  break\n",
        "for sentence in test_data:\n",
        "  print(sentence)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Nj-LW-XM624-",
        "outputId": "b2f901ff-1167-48bc-b7b0-348a730dcc49"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']\n",
            "['&', 'gt', ';', '*', 'The', 'soldier', 'was', 'killed', 'when', 'another', 'avalanche', 'hit', 'an', 'army', 'barracks', 'in', 'the', 'northern', 'area', 'of', 'Sonmarg', ',', 'said', 'a', 'military', 'spokesman', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_dict = {}\n",
        "vocab = set(['', ''])\n",
        "\n",
        "with open('/content/drive/MyDrive/data/glove.twitter.27B.200d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector\n",
        "\n",
        "for sentence, tags in train_data:\n",
        "    vocab.update(sentence)\n",
        "for sentence, tags in dev_data:\n",
        "    vocab.update(sentence)\n",
        "for sentence in test_data:\n",
        "    vocab.update(sentence)\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
        "\n",
        "embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIM))\n",
        "\n",
        "for word in vocab:\n",
        "    index = word_to_ix[word]\n",
        "    if word in embeddings_dict:\n",
        "        vector = embeddings_dict[word]\n",
        "    elif word.lower() in embeddings_dict:\n",
        "        vector = embeddings_dict[word.lower()]\n",
        "    else:\n",
        "        vector = np.random.rand(EMBEDDING_DIM)\n",
        "    embedding_matrix[index] = vector\n",
        "\n",
        "VOCAB_SIZE = len(word_to_ix)"
      ],
      "metadata": {
        "id": "wzXuJnkMayBs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BLSTM2(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, linear_dim, tags_size, lstm_dropout, elu_alpha, embeddings, spelling_embedding_dim):\n",
        "        super(BLSTM2, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embeddings_word = nn.Embedding.from_pretrained(torch.from_numpy(embeddings).float(), freeze=False, padding_idx=word_to_ix[''])\n",
        "        self.embeddings_spelling = nn.Embedding(num_embeddings=5, embedding_dim=spelling_embedding_dim, padding_idx=0)\n",
        "        self.dropout_pre_lstm = nn.Dropout(lstm_dropout)\n",
        "        self.lstm = nn.LSTM(embedding_dim+spelling_embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout_post_lstm = nn.Dropout(lstm_dropout)\n",
        "        self.linear = nn.Linear(hidden_dim * 2, linear_dim)\n",
        "        self.elu = nn.ELU(alpha=elu_alpha)\n",
        "        #self.ReLu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(linear_dim, tags_size)\n",
        "    \n",
        "    def forward(self, x_word, x_spelling):\n",
        "        x1 = self.embeddings_word(x_word)\n",
        "        x2 = self.embeddings_spelling(x_spelling)\n",
        "        x = torch.cat((x1, x2), dim=2).to(device)\n",
        "        x = self.dropout_pre_lstm(x)\n",
        "        \n",
        "        h0 = torch.zeros(2, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(2, x.size(0), self.hidden_dim).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        \n",
        "        out = self.dropout_post_lstm(out)\n",
        "        out = self.linear(out)\n",
        "        # out = self.ReLu(out)\n",
        "        out = self.elu(out)\n",
        "        out = self.linear2(out)\n",
        "    \n",
        "        return out\n"
      ],
      "metadata": {
        "id": "PxFglhGOcF0Q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "cRxZscMwIRa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to predict on a development data loader\n",
        "# Writes the output to a file, i.e. to dev.out\n",
        "def predict_dev2(model, data_loader, fname):\n",
        "    outputs = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y, X_original, X_spelling in data_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            y_pred_scores = model(X, X_spelling)\n",
        "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
        "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
        "\n",
        "            idx = 1\n",
        "            output = []\n",
        "            for i in range(len(y_pred_flat)):\n",
        "                word = X_original[i][0]\n",
        "                pred = ix_to_tag[y_pred_flat[i]]\n",
        "                if word == '':\n",
        "                    break\n",
        "                output.append((idx, word, pred))\n",
        "                idx += 1\n",
        "            outputs.append(output)\n",
        "\n",
        "    with open(fname, 'w') as f:\n",
        "        for i in range(len(outputs)):\n",
        "            for j in range(len(outputs[i])):\n",
        "                idx, word, pred = outputs[i][j]\n",
        "                f.write(f'{idx} {word} {pred}\\n')\n",
        "            if i != len(outputs)-1:\n",
        "                f.write('\\n')\n",
        "\n",
        "# Used to predict on a test data, list of sentences\n",
        "# Writes the output to a file, i.e. to test.out\n",
        "def predict_test2(model, sentences, fname):\n",
        "    outputs = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sentence in sentences:\n",
        "            spelling_sentence = [get_spelling_feature(sentence)]\n",
        "            spelling_sentence = torch.from_numpy(np.array(spelling_sentence, dtype=np.int64)).to(device)\n",
        "            \n",
        "            transformed_sentence = [prepare_sequence(sentence, word_to_ix, use_unk=True)]\n",
        "            transformed_sentence = torch.from_numpy(np.array(transformed_sentence, dtype=np.int64)).to(device)\n",
        "            \n",
        "            y_pred_scores = model(transformed_sentence, spelling_sentence)\n",
        "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
        "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
        "\n",
        "            idx = 1\n",
        "            output = []\n",
        "            for i in range(len(y_pred_flat)):\n",
        "                word = sentence[i]\n",
        "                pred = ix_to_tag[y_pred_flat[i]]\n",
        "                if word == '':\n",
        "                    break\n",
        "                output.append((word, pred))\n",
        "                idx += 1\n",
        "            outputs.append(output)\n",
        "\n",
        "    with open(fname, 'w') as f:\n",
        "        for i in range(len(outputs)):\n",
        "            for j in range(len(outputs[i])):\n",
        "                word, pred = outputs[i][j]\n",
        "                f.write(f'{word} {pred}\\n')\n",
        "            if i != len(outputs)-1:\n",
        "                f.write('\\n')\n",
        "                \n",
        "# Used to predict on a development data loader\n",
        "# Writes statistics to console\n",
        "def predict2(model, data_loader, message):\n",
        "    all_y = []\n",
        "    all_y_pred = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y, X_original, X_spelling in data_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            y_pred_scores = model(X, X_spelling)\n",
        "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
        "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
        "            y_flat = torch.flatten(y).tolist()\n",
        "            \n",
        "            for i in range(len(y_pred_flat)):\n",
        "                if y_flat[i] == tag_to_ix['']:\n",
        "                    break\n",
        "                all_y.append(y_flat[i])\n",
        "                all_y_pred.append(y_pred_flat[i])\n",
        "\n",
        "    print(message, classification_report(all_y, all_y_pred))\n",
        "                \n",
        "# Used to predict on a development data loader\n",
        "# Writes the output to a file for PERL script, i.e. to prediction.txt\n",
        "def predict_perl2(model, data_loader, fname):\n",
        "    outputs = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y, X_original, X_spelling in data_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            y_pred_scores = model(X, X_spelling)\n",
        "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
        "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
        "            y_flat = torch.flatten(y).tolist()\n",
        "\n",
        "            idx = 1\n",
        "            output = []\n",
        "            for i in range(len(y_pred_flat)):\n",
        "                word = X_original[i][0]\n",
        "                gold = ix_to_tag[y_flat[i]]\n",
        "                pred = ix_to_tag[y_pred_flat[i]]\n",
        "                if word == '':\n",
        "                    break\n",
        "                output.append((idx, word, gold, pred))\n",
        "                idx += 1\n",
        "            outputs.append(output)\n",
        "\n",
        "    with open(fname, 'w') as f:\n",
        "        for i in range(len(outputs)):\n",
        "            for j in range(len(outputs[i])):\n",
        "                idx, word, gold, pred = outputs[i][j]\n",
        "                f.write(f'{idx} {word} {gold} {pred}\\n')\n",
        "            if i != len(outputs)-1:\n",
        "                f.write('\\n')"
      ],
      "metadata": {
        "id": "o-mnhHBKb2zh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "nBMgV8YwIUMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NERDataset(train_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "dev_dataset = NERDataset(dev_data)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "FNILlh5qb3wN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BLSTM2(VOCAB_SIZE, EMBEDDING_DIM, LSTM_HIDDEN_DIM, LINEAR_DIM, TAGS_SIZE, LSTM_DROPOUT, ELU_ALPHA,\n",
        "               embedding_matrix, SPELLING_EMBEDDING_DIM).to(device)\n",
        "ratio = float(2000/14483)\n",
        "\n",
        "weights = [0,ratio,1-ratio]\n",
        "class_weights = torch.FloatTensor(weights).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)\n"
      ],
      "metadata": {
        "id": "FSmFBql7b7Re"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)"
      ],
      "metadata": {
        "id": "172jF79vNCWp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "0eanSFwBnfMa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if os.path.isfile('blstm2.pt'):\n",
        "    print('Task 2', 'blstm2.pt exists. Loading existing model...')\n",
        "    model = torch.load('blstm2.pt')\n",
        "    model.to(device)\n",
        "else:\n",
        "    print('Task 2', 'blstm2.pt does not exist. Training a new model...')\n",
        "    total_loss = []\n",
        "    for epoch in range(3):\n",
        "        model.train()\n",
        "        for i, (X, y, X_original, X_spelling) in enumerate(train_loader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            y_pred_scores = model(X, X_spelling)\n",
        "            y_pred = torch.flatten(y_pred_scores, start_dim=0, end_dim=1)\n",
        "            y = torch.flatten(y)\n",
        "            loss = criterion(y_pred, y) \n",
        "            # loss = loss_fn(y_pred,y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss.append(loss.item())\n",
        "        print(f'Epoch {epoch+1} / {NUM_EPOCHS}, training loss: {np.average(total_loss):.5f}, learning rate: {optimizer.param_groups[0][\"lr\"]:.5f}')\n",
        "        total_loss = []\n",
        "        scheduler.step()\n",
        "        predict2(model, dev_loader, f'Epoch {epoch+1} / {NUM_EPOCHS}')\n"
      ],
      "metadata": {
        "id": "OFDiU10wcNG-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4b69000d-b630-48e7-aa44-6aad822c622d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 2 blstm2.pt does not exist. Training a new model...\n",
            "Epoch 1 / 100, training loss: 0.22706, learning rate: 0.01000\n",
            "Epoch 1 / 100               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.97      0.97     14483\n",
            "           2       0.63      0.70      0.66      1250\n",
            "\n",
            "    accuracy                           0.94     15733\n",
            "   macro avg       0.80      0.83      0.82     15733\n",
            "weighted avg       0.95      0.94      0.95     15733\n",
            "\n",
            "Epoch 2 / 100, training loss: 0.17293, learning rate: 0.01000\n",
            "Epoch 2 / 100               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.96      0.99      0.98     14483\n",
            "           2       0.85      0.53      0.65      1250\n",
            "\n",
            "    accuracy                           0.96     15733\n",
            "   macro avg       0.91      0.76      0.82     15733\n",
            "weighted avg       0.95      0.96      0.95     15733\n",
            "\n",
            "Epoch 3 / 100, training loss: 0.15392, learning rate: 0.01000\n",
            "Epoch 3 / 100               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.99      0.98     14483\n",
            "           2       0.81      0.61      0.70      1250\n",
            "\n",
            "    accuracy                           0.96     15733\n",
            "   macro avg       0.89      0.80      0.84     15733\n",
            "weighted avg       0.96      0.96      0.96     15733\n",
            "\n",
            "CPU times: user 8min 28s, sys: 21.4 s, total: 8min 50s\n",
            "Wall time: 8min 49s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UPcXEqaNarMK"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}